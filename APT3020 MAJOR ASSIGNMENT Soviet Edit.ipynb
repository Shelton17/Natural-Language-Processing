{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives \n",
    " - Map how current News websites are to trending twitter topics\n",
    " - Websites used\n",
    "     - BBC News\n",
    "     - Telegraph\n",
    "     - The Guardian\n",
    "     - The Independent\n",
    "     - Bloomberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "from wordcloud import WordCloud\n",
    "from pathlib import Path\n",
    "# handling images\n",
    "# https://stackoverflow.com/questions/35286540/display-an-image-with-python\n",
    "import matplotlib.image as mpimg\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_03 : project Guternburg  pick 2 books and do 2 analyses and create a barplot of the top 30 popular words\n",
    "whether they use the same words and create a barplot of the top 30 used words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Good and Evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_blob=TextBlob(Path('BeyondGoodAndEvil.txt').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_items=BGAE_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BGAE_items =[item for item in BGAE_items if item[0] not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_sorted_items=sorted(BGAE_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_top20=BGAE_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_df=pd.DataFrame(BGAE_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BGAE_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver df to tuple\n",
    "BGAE_tuples = [tuple(x) for x in BGAE_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_wordcloud=WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BGAE_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGAE_wordcloud_1 = BGAE_wordcloud.to_file('BGAE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(BGAE_wordcloud_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dracula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_blob=TextBlob(Path('Dracula.txt').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_items=DRACULA_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "DRACULA_items =[item for item in DRACULA_items if item[0] not in DRACULA_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_sorted_items=sorted(DRACULA_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_top20=DRACULA_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_df=pd.DataFrame(DRACULA_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=DRACULA_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver df to tuple\n",
    "DRACULA_tuples = [tuple(x) for x in DRACULA_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_wordcloud=WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(DRACULA_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRACULA_wordcloud_1 = DRACULA_wordcloud.to_file('BGAE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(DRACULA_wordcloud_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Objectives\n",
    "   + stream tweets for at least 5 sessions\n",
    "   + create wordcloud for each session\n",
    "   + create a bar chart for each session\n",
    "   + Get from news media - maps with what is current \n",
    "   + which chart is almost the same with the twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(keys.access_token,keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#https://stackoverflow.com/questions/30721567/limit-tweepy-stream-to-a-specific-number\n",
    "#https://www.storybench.org/how-to-collect-tweets-from-the-twitter-streaming-api-using-python/\n",
    "\n",
    "# authorization tokens\n",
    "#consumer_key = \"[insert your key here]\"\n",
    "#consumer_secret = \"[insert your secret here]\"\n",
    "#access_key = \"[insert your key here]\"\n",
    "#access_secret = \"[insert your secret here]\"\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tweet_count=0\n",
    "        self.TWEET_LIMIT = 19 \n",
    "    def on_status(self, status):\n",
    "        #self.tweet_count=0\n",
    "        #self.TWEET_LIMIT = limit\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"tweet1.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (text))\n",
    "        self.tweet_count += 1\n",
    "        #return self.tweet_count <= self.TWEET_LIMIT\n",
    "        if self.tweet_count <= self.TWEET_LIMIT:\n",
    "            return True\n",
    "        else:\n",
    "            # close file? nah\n",
    "            # https://stackoverflow.com/questions/33498975/unable-to-stop-streaming-in-tweepy-after-one-minute\n",
    "            # returining false to close stream\n",
    "            return False\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)\n",
    "    auth.set_access_token(keys.access_token,keys.access_token_secret)\n",
    "    api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "  #  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  #  auth.set_access_token(access_key, access_secret)\n",
    "   # api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"tweet1.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"United Kingdom\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_text= Path('tweet1.txt').read_text(errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor options\n",
    "\n",
    "Option Name\tOption Short Code\n",
    "\n",
    "URL   \tp.OPT.URL\n",
    "\n",
    "Mention\tp.OPT.MENTION\n",
    "\n",
    "Hashtag\tp.OPT.HASHTAG\n",
    "\n",
    "Reserved Words\tp.OPT.RESERVED\n",
    "\n",
    "Emoji\tp.OPT.EMOJI\n",
    "\n",
    "Smiley\tp.OPT.SMILEY\n",
    "\n",
    "Number\tp.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Preprocessor options enabled\n",
    "p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG,p.OPT.MENTION,p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet1_text=p.clean(tweet1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_blob=TextBlob(clean_tweet1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_items=tweet1_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the stopword list that were not blocked by the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "tweet1_stopwords_list = ['RT','ðÿ‡¬ðÿ‡§ðÿ‡ºðÿ‡¸\t','ukâ€\t','“','rt','date,user,is_retweet,is_quote,text,quoted_text','@','United','Kingdom']\n",
    "tweet1_stop_words.extend(tweet1_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "tweet1_items =[item for item in tweet1_items if item[0] not in tweet1_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_sorted_items=sorted(tweet1_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_top20=tweet1_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_df=pd.DataFrame(tweet1_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=tweet1_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_tuples = [tuple(x) for x in tweet1_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(tweet1_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1_wordcloud_1 = tweet1_wordcloud1.to_file('tweet1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tweet1_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BBC News Session 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_URL = 'https://www.bbc.com/news/uk'\n",
    "BBC_page = requests.get(BBC_URL)\n",
    "BBC_content = BBC_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(BBC_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_blob=TextBlob(BBC_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_items=BBC_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "BBC_stopwords_list = ['optionsshare','toolsfacebooktwittershareview','morenextarticle','linkread',\n",
    "                      'links.posted','hours','minutes','bbc','share','postcopy','news','contentnew']\n",
    "BBC_stopwords.extend(BBC_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BBC_items =[item for item in BBC_items if item[0] not in BBC_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_sorted_items=sorted(BBC_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_sorted_items[len(BBC_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BBC_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "BBC_top20=BBC_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_df=pd.DataFrame(BBC_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BBC_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_tuples = [tuple(x) for x in BBC_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BBC_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BBC_wordcloud_1 = BBC_wordcloud1.to_file('BBC1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(BBC_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraph Comparison Session 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_URL = 'https://www.Telegraph.co.uk/'\n",
    "Tele_page = requests.get(Tele_URL)\n",
    "Tele_content = Tele_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Tele_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_blob=TextBlob(Tele_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_items=Tele_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "Tele_stopwords_list = ['’','–','‘','Telegraph']\n",
    "Tele_stopwords.extend(Tele_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "Tele_items =[item for item in Tele_items if item[0] not in Tele_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_sorted_items=sorted(Tele_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_sorted_items[len(Tele_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tele_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "Tele_top20=Tele_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_df=pd.DataFrame(Tele_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=Tele_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_tuples = [tuple(x) for x in Tele_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(Tele_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tele_wordcloud_1 = Tele_wordcloud1.to_file('Tele1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Tele_wordcloud1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(keys.access_token,keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#https://stackoverflow.com/questions/30721567/limit-tweepy-stream-to-a-specific-number\n",
    "#https://www.storybench.org/how-to-collect-tweets-from-the-twitter-streaming-api-using-python/\n",
    "\n",
    "# authorization tokens\n",
    "#consumer_key = \"[insert your key here]\"\n",
    "#consumer_secret = \"[insert your secret here]\"\n",
    "#access_key = \"[insert your key here]\"\n",
    "#access_secret = \"[insert your secret here]\"\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tweet_count=0\n",
    "        self.TWEET_LIMIT = 19 \n",
    "    def on_status(self, status):\n",
    "        #self.tweet_count=0\n",
    "        #self.TWEET_LIMIT = limit\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"tweet2.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (text))\n",
    "        self.tweet_count += 1\n",
    "        #return self.tweet_count <= self.TWEET_LIMIT\n",
    "        if self.tweet_count <= self.TWEET_LIMIT:\n",
    "            return True\n",
    "        else:\n",
    "            # close file? nah\n",
    "            # https://stackoverflow.com/questions/33498975/unable-to-stop-streaming-in-tweepy-after-one-minute\n",
    "            # returining false to close stream\n",
    "            return False\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)\n",
    "    auth.set_access_token(keys.access_token,keys.access_token_secret)\n",
    "    api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "  #  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  #  auth.set_access_token(access_key, access_secret)\n",
    "   # api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"tweet2.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"United Kingdom\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_text= Path('tweet2.txt').read_text(errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor options\n",
    "\n",
    "Option Name\tOption Short Code\n",
    "\n",
    "URL   \tp.OPT.URL\n",
    "\n",
    "Mention\tp.OPT.MENTION\n",
    "\n",
    "Hashtag\tp.OPT.HASHTAG\n",
    "\n",
    "Reserved Words\tp.OPT.RESERVED\n",
    "\n",
    "Emoji\tp.OPT.EMOJI\n",
    "\n",
    "Smiley\tp.OPT.SMILEY\n",
    "\n",
    "Number\tp.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Preprocessor options enabled\n",
    "p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG,p.OPT.MENTION,p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet2_text=p.clean(tweet2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_blob=TextBlob(clean_tweet2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_items=tweet2_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the stopword list that were not blocked by the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "tweet2_stopwords_list = ['RT','ðÿ‡¬ðÿ‡§ðÿ‡ºðÿ‡¸\t','ukâ€\t','“','rt','date,user,is_retweet,is_quote,text,quoted_text','@','United','Kingdom']\n",
    "tweet2_stop_words.extend(tweet2_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "tweet2_items =[item for item in tweet2_items if item[0] not in tweet2_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_sorted_items=sorted(tweet2_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_top20=tweet2_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_df=pd.DataFrame(tweet2_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=tweet2_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tuble for word cloud to work\n",
    "tweet2_tuples = [tuple(x) for x in tweet2_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_wordcloud = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(tweet2_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2_wordcloud_1 = tweet2_wordcloud.to_file('tweet2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tweet2_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BBC News Session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_URL = 'https://www.bbc.com/news/uk'\n",
    "BBC2_page = requests.get(BBC2_URL)\n",
    "BBC2_content = BBC2_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(BBC2_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_blob=TextBlob(BBC2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_items=BBC2_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "BBC2_stopwords_list = ['optionsshare','toolsfacebooktwittershareview','morenextarticle','linkread',\n",
    "                      'links.posted','hours','minutes','bbc','share','postcopy','news','contentnew']\n",
    "BBC2_stopwords.extend(BBC2_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BBC2_items =[item for item in BBC2_items if item[0] not in BBC2_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_sorted_items=sorted(BBC2_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_sorted_items[len(BBC2_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BBC2_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "BBC2_top20=BBC2_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_df=pd.DataFrame(BBC2_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BBC2_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_tuples = [tuple(x) for x in BBC2_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC2_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BBC2_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BBC2_wordcloud_1 = BBC2_wordcloud1.to_file('BBC2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(BBC2_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraph Comparison Session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_URL = 'https://www.Telegraph.co.uk/'\n",
    "Tele2_page = requests.get(Tele2_URL)\n",
    "Tele2_content = Tele2_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Tele2_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_blob=TextBlob(Tele2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_items=Tele2_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "Tele2_stopwords_list = ['’','–','‘','Telegraph']\n",
    "Tele2_stopwords.extend(Tele2_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "Tele2_items =[item for item in Tele2_items if item[0] not in Tele2_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_sorted_items=sorted(Tele2_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_sorted_items[len(Tele2_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tele2_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "Tele2_top20=Tele2_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_df=pd.DataFrame(Tele2_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=Tele2_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_tuples = [tuple(x) for x in Tele2_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele2_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(Tele2_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tele2_wordcloud_1 = Tele2_wordcloud1.to_file('Tele2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Tele2_wordcloud1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(keys.access_token,keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#https://stackoverflow.com/questions/30721567/limit-tweepy-stream-to-a-specific-number\n",
    "#https://www.storybench.org/how-to-collect-tweets-from-the-twitter-streaming-api-using-python/\n",
    "\n",
    "# authorization tokens\n",
    "#consumer_key = \"[insert your key here]\"\n",
    "#consumer_secret = \"[insert your secret here]\"\n",
    "#access_key = \"[insert your key here]\"\n",
    "#access_secret = \"[insert your secret here]\"\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tweet_count=0\n",
    "        self.TWEET_LIMIT = 19 \n",
    "    def on_status(self, status):\n",
    "        #self.tweet_count=0\n",
    "        #self.TWEET_LIMIT = limit\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"tweet3.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (text))\n",
    "        self.tweet_count += 1\n",
    "        #return self.tweet_count <= self.TWEET_LIMIT\n",
    "        if self.tweet_count <= self.TWEET_LIMIT:\n",
    "            return True\n",
    "        else:\n",
    "            # close file? nah\n",
    "            # https://stackoverflow.com/questions/33498975/unable-to-stop-streaming-in-tweepy-after-one-minute\n",
    "            # returining false to close stream\n",
    "            return False\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)\n",
    "    auth.set_access_token(keys.access_token,keys.access_token_secret)\n",
    "    api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "  #  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  #  auth.set_access_token(access_key, access_secret)\n",
    "   # api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"tweet3.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"United Kingdom\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_text= Path('tweet3.txt').read_text(errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor options\n",
    "\n",
    "Option Name\tOption Short Code\n",
    "\n",
    "URL   \tp.OPT.URL\n",
    "\n",
    "Mention\tp.OPT.MENTION\n",
    "\n",
    "Hashtag\tp.OPT.HASHTAG\n",
    "\n",
    "Reserved Words\tp.OPT.RESERVED\n",
    "\n",
    "Emoji\tp.OPT.EMOJI\n",
    "\n",
    "Smiley\tp.OPT.SMILEY\n",
    "\n",
    "Number\tp.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Preprocessor options enabled\n",
    "p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG,p.OPT.MENTION,p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet3_text=p.clean(tweet3_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_blob=TextBlob(clean_tweet3_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_items=tweet3_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the stopword list that were not blocked by the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "tweet3_stopwords_list = ['RT','ðÿ‡¬ðÿ‡§ðÿ‡ºðÿ‡¸\t','ukâ€\t','“','rt','date,user,is_retweet,is_quote,text,quoted_text','@','United','Kingdom']\n",
    "tweet3_stop_words.extend(tweet3_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "tweet3_items =[item for item in tweet3_items if item[0] not in tweet3_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_sorted_items=sorted(tweet3_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_top20=tweet3_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_df=pd.DataFrame(tweet3_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=tweet3_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tuble for word cloud to work\n",
    "tweet3_tuples = [tuple(x) for x in tweet3_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_wordcloud = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(tweet3_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet3_wordcloud_1 = tweet3_wordcloud.to_file('tweet3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tweet3_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BBC News Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_URL = 'https://www.bbc.com/news/uk'\n",
    "BBC3_page = requests.get(BBC3_URL)\n",
    "BBC3_content = BBC3_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(BBC3_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_blob=TextBlob(BBC3_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_items=BBC3_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "BBC3_stopwords_list = ['optionsshare','toolsfacebooktwittershareview','morenextarticle','linkread',\n",
    "                      'links.posted','hours','minutes','bbc','share','postcopy','news','contentnew']\n",
    "BBC3_stopwords.extend(BBC3_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BBC3_items =[item for item in BBC3_items if item[0] not in BBC3_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_sorted_items=sorted(BBC3_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_sorted_items[len(BBC3_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BBC3_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "BBC3_top20=BBC3_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_df=pd.DataFrame(BBC3_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BBC3_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_tuples = [tuple(x) for x in BBC3_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC3_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BBC3_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BBC3_wordcloud_1 = BBC3_wordcloud1.to_file('BBC3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(BBC3_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraph Comparison Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_URL = 'https://www.Telegraph.co.uk/'\n",
    "Tele3_page = requests.get(Tele3_URL)\n",
    "Tele3_content = Tele3_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Tele3_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_blob=TextBlob(Tele3_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_items=Tele3_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "Tele3_stopwords_list = ['’','–','‘','Tele3graph']\n",
    "Tele3_stopwords.extend(Tele3_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "Tele3_items =[item for item in Tele3_items if item[0] not in Tele3_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_sorted_items=sorted(Tele3_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_sorted_items[len(Tele3_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tele3_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "Tele3_top20=Tele3_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_df=pd.DataFrame(Tele3_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=Tele3_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_tuples = [tuple(x) for x in Tele3_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele3_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(Tele3_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tele3_wordcloud_1 = Tele3_wordcloud1.to_file('Tele3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Tele3_wordcloud1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(keys.access_token,keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#https://stackoverflow.com/questions/30721567/limit-tweepy-stream-to-a-specific-number\n",
    "#https://www.storybench.org/how-to-collect-tweets-from-the-twitter-streaming-api-using-python/\n",
    "\n",
    "# authorization tokens\n",
    "#consumer_key = \"[insert your key here]\"\n",
    "#consumer_secret = \"[insert your secret here]\"\n",
    "#access_key = \"[insert your key here]\"\n",
    "#access_secret = \"[insert your secret here]\"\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tweet_count=0\n",
    "        self.TWEET_LIMIT = 19 \n",
    "    def on_status(self, status):\n",
    "        #self.tweet_count=0\n",
    "        #self.TWEET_LIMIT = limit\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"tweet4.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (text))\n",
    "        self.tweet_count += 1\n",
    "        #return self.tweet_count <= self.TWEET_LIMIT\n",
    "        if self.tweet_count <= self.TWEET_LIMIT:\n",
    "            return True\n",
    "        else:\n",
    "            # close file? nah\n",
    "            # https://stackoverflow.com/questions/33498975/unable-to-stop-streaming-in-tweepy-after-one-minute\n",
    "            # returining false to close stream\n",
    "            return False\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)\n",
    "    auth.set_access_token(keys.access_token,keys.access_token_secret)\n",
    "    api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "  #  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  #  auth.set_access_token(access_key, access_secret)\n",
    "   # api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"tweet4.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"United Kingdom\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_text= Path('tweet4.txt').read_text(errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor options\n",
    "\n",
    "Option Name\tOption Short Code\n",
    "\n",
    "URL   \tp.OPT.URL\n",
    "\n",
    "Mention\tp.OPT.MENTION\n",
    "\n",
    "Hashtag\tp.OPT.HASHTAG\n",
    "\n",
    "Reserved Words\tp.OPT.RESERVED\n",
    "\n",
    "Emoji\tp.OPT.EMOJI\n",
    "\n",
    "Smiley\tp.OPT.SMILEY\n",
    "\n",
    "Number\tp.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Preprocessor options enabled\n",
    "p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG,p.OPT.MENTION,p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet4_text=p.clean(tweet4_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_blob=TextBlob(clean_tweet4_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_items=tweet4_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the stopword list that were not blocked by the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "tweet4_stopwords_list = ['RT','ðÿ‡¬ðÿ‡§ðÿ‡ºðÿ‡¸\t','ukâ€\t','“','rt','date,user,is_retweet,is_quote,text,quoted_text','@','United','Kingdom']\n",
    "tweet4_stop_words.extend(tweet4_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "tweet4_items =[item for item in tweet4_items if item[0] not in tweet4_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_sorted_items=sorted(tweet4_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_top20=tweet4_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_df=pd.DataFrame(tweet4_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=tweet4_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tuble for word cloud to work\n",
    "tweet4_tuples = [tuple(x) for x in tweet4_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_wordcloud = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(tweet4_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet4_wordcloud_1 = tweet4_wordcloud.to_file('tweet4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tweet4_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BBC News Session 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_URL = 'https://www.bbc.com/news/uk'\n",
    "BBC4_page = requests.get(BBC4_URL)\n",
    "BBC4_content = BBC4_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(BBC4_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_blob=TextBlob(BBC4_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_items=BBC4_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "BBC4_stopwords_list = ['optionsshare','toolsfacebooktwittershareview','morenextarticle','linkread',\n",
    "                      'links.posted','hours','minutes','bbc','share','postcopy','news','contentnew']\n",
    "BBC4_stopwords.extend(BBC4_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BBC4_items =[item for item in BBC4_items if item[0] not in BBC4_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_sorted_items=sorted(BBC4_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_sorted_items[len(BBC4_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BBC4_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "BBC4_top20=BBC4_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_df=pd.DataFrame(BBC4_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BBC4_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_tuples = [tuple(x) for x in BBC4_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC4_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BBC4_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BBC4_wordcloud_1 = BBC4_wordcloud1.to_file('BBC4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(BBC4_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraph Comparison Session 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_URL = 'https://www.Telegraph.co.uk/'\n",
    "Tele4_page = requests.get(Tele4_URL)\n",
    "Tele4_content = Tele4_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Tele4_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_blob=TextBlob(Tele4_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_items=Tele4_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "Tele4_stopwords_list = ['’','–','‘','Tele4graph']\n",
    "Tele4_stopwords.extend(Tele4_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "Tele4_items =[item for item in Tele4_items if item[0] not in Tele4_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_sorted_items=sorted(Tele4_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_sorted_items[len(Tele4_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tele4_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "Tele4_top20=Tele4_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_df=pd.DataFrame(Tele4_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=Tele4_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_tuples = [tuple(x) for x in Tele4_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele4_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(Tele4_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tele4_wordcloud_1 = Tele4_wordcloud1.to_file('Tele4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Tele4_wordcloud1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(keys.access_token,keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#https://stackoverflow.com/questions/30721567/limit-tweepy-stream-to-a-specific-number\n",
    "#https://www.storybench.org/how-to-collect-tweets-from-the-twitter-streaming-api-using-python/\n",
    "\n",
    "# authorization tokens\n",
    "#consumer_key = \"[insert your key here]\"\n",
    "#consumer_secret = \"[insert your secret here]\"\n",
    "#access_key = \"[insert your key here]\"\n",
    "#access_secret = \"[insert your secret here]\"\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tweet_count=0\n",
    "        self.TWEET_LIMIT = 19 \n",
    "    def on_status(self, status):\n",
    "        #self.tweet_count=0\n",
    "        #self.TWEET_LIMIT = limit\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"tweet5.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (text))\n",
    "        self.tweet_count += 1\n",
    "        #return self.tweet_count <= self.TWEET_LIMIT\n",
    "        if self.tweet_count <= self.TWEET_LIMIT:\n",
    "            return True\n",
    "        else:\n",
    "            # close file? nah\n",
    "            # https://stackoverflow.com/questions/33498975/unable-to-stop-streaming-in-tweepy-after-one-minute\n",
    "            # returining false to close stream\n",
    "            return False\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth=tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)\n",
    "    auth.set_access_token(keys.access_token,keys.access_token_secret)\n",
    "    api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "  #  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  #  auth.set_access_token(access_key, access_secret)\n",
    "   # api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"tweet5.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"United Kingdom\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_text= Path('tweet5.txt').read_text(errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor options\n",
    "\n",
    "Option Name\tOption Short Code\n",
    "\n",
    "URL   \tp.OPT.URL\n",
    "\n",
    "Mention\tp.OPT.MENTION\n",
    "\n",
    "Hashtag\tp.OPT.HASHTAG\n",
    "\n",
    "Reserved Words\tp.OPT.RESERVED\n",
    "\n",
    "Emoji\tp.OPT.EMOJI\n",
    "\n",
    "Smiley\tp.OPT.SMILEY\n",
    "\n",
    "Number\tp.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Preprocessor options enabled\n",
    "p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG,p.OPT.MENTION,p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet5_text=p.clean(tweet5_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_blob=TextBlob(clean_tweet5_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_items=tweet5_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the stopword list that were not blocked by the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "tweet5_stopwords_list = ['RT','ðÿ‡¬ðÿ‡§ðÿ‡ºðÿ‡¸\t','ukâ€\t','“','rt','date,user,is_retweet,is_quote,text,quoted_text','@','United','Kingdom']\n",
    "tweet5_stop_words.extend(tweet5_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "tweet5_items =[item for item in tweet5_items if item[0] not in tweet5_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_sorted_items=sorted(tweet5_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_top20=tweet5_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_df=pd.DataFrame(tweet5_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=tweet5_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tuble for word cloud to work\n",
    "tweet5_tuples = [tuple(x) for x in tweet5_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_wordcloud = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(tweet5_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet5_wordcloud_1 = tweet5_wordcloud.to_file('tweet5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(tweet5_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BBC News Session 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_URL = 'https://www.bbc.com/news/uk'\n",
    "BBC5_page = requests.get(BBC5_URL)\n",
    "BBC5_content = BBC5_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(BBC5_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_blob=TextBlob(BBC5_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_items=BBC5_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "BBC5_stopwords_list = ['optionsshare','toolsfacebooktwittershareview','morenextarticle','linkread',\n",
    "                      'links.posted','hours','minutes','bbc','share','postcopy','news','contentnew']\n",
    "BBC5_stopwords.extend(BBC5_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "BBC5_items =[item for item in BBC5_items if item[0] not in BBC5_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_sorted_items=sorted(BBC5_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_sorted_items[len(BBC5_sorted_items)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BBC5_sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "BBC5_top20=BBC5_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_df=pd.DataFrame(BBC5_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=BBC5_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_tuples = [tuple(x) for x in BBC5_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC5_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(BBC5_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BBC5_wordcloud_1 = BBC5_wordcloud1.to_file('BBC5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(BBC5_wordcloud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraph Comparison  Session 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_URL = 'https://www.Telegraph.co.uk/'\n",
    "Tele5_page = requests.get(Tele5_URL)\n",
    "Tele5_content = Tele5_page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Tele5_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_text = soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_blob=TextBlob(Tele5_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_items=Tele5_blob.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding words to the stopword list that were not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "Tele5_stopwords_list = ['’','–','‘','Tele5graph']\n",
    "Tele5_stopwords.extend(Tele5_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimiate stop words\n",
    "Tele5_items =[item for item in Tele5_items if item[0] not in Tele5_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_sorted_items=sorted(Tele5_items, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the array to  20 items\n",
    "Tele5_top20=Tele5_sorted_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_df=pd.DataFrame(Tele5_top20, columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=Tele5_df.plot.bar(x='words',y='count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_tuples = [tuple(x) for x in Tele5_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tele5_wordcloud1 = WordCloud(width = 2000, height= 2000, prefer_horizontal=0.5, min_font_size=10, colormap='prism', background_color='white').generate_from_frequencies(dict(Tele_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tele5_wordcloud_1 = Tele5_wordcloud1.to_file('Tele5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Tele5_wordcloud1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud supports that twitter is more current than news media websites as the words do not occur at the same frequency, However this comparison may be better suited with the use of the trends API which can be narrowed down to a particular region. The streamlistener api would be best suited for global events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
